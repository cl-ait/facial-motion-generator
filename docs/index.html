<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <meta name="title" content="Facial Motion Generator - Audio-driven Real-time Facial Expression Generation">
  <meta name="description" content="A lightweight audio-driven facial expression generation system for 3D avatars. Real-time inference at 0.62ms/frame with 52-dimensional ARKit BlendShape output.">
  <meta name="keywords" content="facial expression, blendshape, audio-driven, real-time, 3D avatar, ARKit, deep learning, cross-attention">
  <meta name="author" content="Junki Ichikawa, Ryoko Tokuhisa">
  <meta name="robots" content="index, follow">
  <meta name="language" content="Japanese">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Aichi Institute of Technology / RIKEN">
  <meta property="og:title" content="Facial Motion Generator">
  <meta property="og:description" content="A lightweight audio-driven facial expression generation system for 3D avatars.">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="3Dアバターに適用可能な音声駆動型リアルタイム表情生成システム">
  <meta name="citation_author" content="Ichikawa, Junki">
  <meta name="citation_author" content="Tokuhisa, Ryoko">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="言語処理学会 第32回年次大会 (NLP2026)">

  <title>Facial Motion Generator - Audio-driven Real-time Facial Expression Generation</title>

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Noto+Sans+JP:wght@400;500;700&display=swap" rel="stylesheet">

  <style>
    :root {
      --primary-color: #2563eb;
      --text-color: #1f2937;
      --bg-color: #ffffff;
      --section-bg: #f3f4f6;
      --border-color: #e5e7eb;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Inter', 'Noto Sans JP', sans-serif;
      color: var(--text-color);
      line-height: 1.6;
      background: var(--bg-color);
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 0 20px;
    }

    /* Hero Section */
    .hero {
      padding: 60px 0 40px;
      text-align: center;
    }

    .hero h1 {
      font-size: 2.5rem;
      font-weight: 700;
      margin-bottom: 20px;
      color: var(--text-color);
    }

    .authors {
      font-size: 1.1rem;
      color: #4b5563;
      margin-bottom: 10px;
    }

    .authors a {
      color: var(--primary-color);
      text-decoration: none;
    }

    .affiliations {
      font-size: 0.95rem;
      color: #6b7280;
      margin-bottom: 25px;
    }

    .conference {
      display: inline-block;
      background: var(--primary-color);
      color: white;
      padding: 6px 16px;
      border-radius: 20px;
      font-size: 0.9rem;
      font-weight: 500;
      margin-bottom: 30px;
    }

    /* Links */
    .links {
      display: flex;
      justify-content: center;
      gap: 15px;
      flex-wrap: wrap;
      margin-bottom: 40px;
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 20px;
      border-radius: 8px;
      text-decoration: none;
      font-weight: 500;
      font-size: 0.95rem;
      transition: all 0.2s;
    }

    .btn-primary {
      background: var(--text-color);
      color: white;
    }

    .btn-primary:hover {
      background: #374151;
    }

    .btn-secondary {
      background: white;
      color: var(--text-color);
      border: 1px solid var(--border-color);
    }

    .btn-secondary:hover {
      border-color: var(--primary-color);
      color: var(--primary-color);
    }

    .btn-disabled {
      background: #e5e7eb;
      color: #9ca3af;
      border: 1px solid #d1d5db;
      cursor: not-allowed;
    }

    .btn-disabled:hover {
      background: #e5e7eb;
      color: #9ca3af;
    }

    /* Sections */
    section {
      padding: 50px 0;
    }

    section.alt {
      background: var(--section-bg);
    }

    h2 {
      font-size: 1.75rem;
      font-weight: 600;
      margin-bottom: 20px;
      text-align: center;
    }

    /* Abstract */
    .abstract {
      text-align: justify;
      max-width: 800px;
      margin: 0 auto;
    }

    /* Results Table */
    .results-table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }

    .results-table th,
    .results-table td {
      padding: 12px 15px;
      text-align: center;
      border-bottom: 1px solid var(--border-color);
    }

    .results-table th {
      background: var(--section-bg);
      font-weight: 600;
    }

    .results-table tr:hover {
      background: #f9fafb;
    }

    /* Architecture */
    .architecture-img {
      width: 100%;
      max-width: 800px;
      display: block;
      margin: 0 auto;
    }

    /* BibTeX */
    .bibtex {
      background: var(--section-bg);
      padding: 20px;
      border-radius: 8px;
      font-family: monospace;
      font-size: 0.85rem;
      overflow-x: auto;
      white-space: pre-wrap;
      position: relative;
    }

    .copy-btn {
      position: absolute;
      top: 10px;
      right: 10px;
      padding: 6px 12px;
      background: white;
      border: 1px solid var(--border-color);
      border-radius: 4px;
      cursor: pointer;
      font-size: 0.8rem;
    }

    .copy-btn:hover {
      background: var(--section-bg);
    }

    /* Footer */
    footer {
      padding: 30px 0;
      text-align: center;
      font-size: 0.9rem;
      color: #6b7280;
      border-top: 1px solid var(--border-color);
    }

    footer a {
      color: var(--primary-color);
      text-decoration: none;
    }

    /* Responsive */
    @media (max-width: 600px) {
      .hero h1 {
        font-size: 1.75rem;
      }

      .links {
        flex-direction: column;
        align-items: center;
      }

      .btn {
        width: 100%;
        max-width: 250px;
        justify-content: center;
      }
    }
  </style>
</head>
<body>

  <main>
    <!-- Hero -->
    <section class="hero">
      <div class="container">
        <h1>Facial Motion Generator</h1>
        <p class="subtitle" style="font-size: 1.1rem; color: #4b5563; margin-bottom: 20px;">
          3Dアバターに適用可能な音声駆動型リアルタイム表情生成システム
        </p>

        <div class="authors">
          <span>市川淳貴<sup>1</sup></span>,
          <span>徳久良子<sup>1,2</sup></span>
        </div>

        <div class="affiliations">
          <sup>1</sup>愛知工業大学,
          <sup>2</sup>理化学研究所
        </div>

        <div class="conference">NLP2026</div>

        <div class="links">
          <a href="https://github.com/cl-ait/facial-motion-generator" class="btn btn-primary">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
              <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
            </svg>
            Code
          </a>
          <span class="btn btn-disabled" title="Paper not yet published">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
              <path d="M14,2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18A2,2 0 0,0 20,20V8L14,2M18,20H6V4H13V9H18V20Z"/>
            </svg>
            Paper (To appear)
          </span>
        </div>
      </div>
    </section>

    <!-- Abstract -->
    <section class="alt">
      <div class="container">
        <h2>概要</h2>
        <div class="abstract">
          <p>
            近年、3Dアバターを介した対話が普及する一方、音声に同期した表情生成は遅延や実装依存が課題である。
            本研究では、音声のみから表情動作の制御値を逐次推定し、ローカルでリアルタイム動作する<strong>Facial Motion Generator</strong>を提案する。
            異なる音声特徴量と軽量なCross-Attentionモデルを用いた設計により、
            Audio2Face-3Dと比較して単一話者データでMAE（平均絶対誤差）を0.147→0.111に低減し、
            推論時間を0.62ms/frameへ高速化した。
          </p>
        </div>
      </div>
    </section>

    <!-- Demo -->
    <section>
      <div class="container">
        <h2>推論デモ</h2>
        <img src="static/images/inference_image.svg" alt="Inference Demo with 3D Avatar" class="architecture-img">
        <p style="text-align: center; margin-top: 15px; color: #6b7280; font-size: 0.9rem;">
          リアルタイム推論による3Dアバターの表情駆動例（3Dアバター: <a href="https://github.com/mmdagent-ex/gene" target="_blank">Gene</a>）
        </p>
      </div>
    </section>

    <!-- Architecture -->
    <section class="alt">
      <div class="container">
        <h2>システム構成</h2>
        <img src="static/images/architecture_image.svg" alt="Facial Motion Generator Architecture" class="architecture-img">
      </div>
    </section>

    <!-- Method -->
    <section>
      <div class="container">
        <h2>提案手法</h2>
        <div class="abstract">
          <p>
            本研究では、音声のみを入力として3Dアバターの表情を駆動するための表情動作の制御値（52次元BlendShape係数）をリアルタイムに推定する<strong>Facial Motion Generator</strong>を提案します。
          </p>
          <p style="margin-top: 15px;">
            <strong>Audio Encoder：</strong>音声からLog-Mel特徴量（短期的なスペクトル情報）とeGeMAPS特徴量（声の高さや強さなど長期的な韻律情報）を抽出し、
            発音に由来する口の形と抑揚に由来する表情変化の両方を表現します。
          </p>
          <p style="margin-top: 15px;">
            <strong>表情生成モデル：</strong>52個のBlendShape要素それぞれに対応した表情特徴量（学習可能なパラメータ）をQueryとし、
            Cross-Attentionにより音声特徴量のどの部分に注目すべきかを計算します。
            これにより、音声特徴量の成分が口元・眉・目などどの表情要素に対応するかをモデルが学習し、軽量かつ高速な推論を実現しています。
          </p>
        </div>
      </div>
    </section>

    <!-- Background -->
    <section class="alt">
      <div class="container">
        <h2>背景と課題</h2>
        <div class="abstract">
          <p>
            3Dアバターを用いた配信や対話エージェントの普及により、人と人（もしくは人とAI）が仮想的なキャラクターを通してコミュニケーションする機会が増えています。
            こうした場面では、発話内容だけでなく、うなずきや口の動き、表情といった<strong>非言語情報</strong>が対話の自然さや理解のしやすさに大きく関わります。
          </p>
          <p style="margin-top: 15px;">
            アバターの表情を適切に制御するためには、口の開閉や眉の上げ下げといった動きを数値として表した「表情動作の制御値」を各時刻において逐次推定する必要があります。
            NVIDIAのAudio2Faceは高品質な表情生成を実現していますが、一部はAPI経由で動作するため通信に起因する遅延が生じる可能性があり、
            またシステムがオープンではない実装に依存する場合、ローカル環境における再現や改良が困難という課題があります。
          </p>
        </div>
      </div>
    </section>

    <!-- Results -->
    <section>
      <div class="container">
        <h2>実験結果</h2>
        <table class="results-table">
          <thead>
            <tr>
              <th>モデル</th>
              <th>MSE ↓</th>
              <th>MAE ↓</th>
              <th>パラメータ数</th>
              <th>推論時間</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Audio2Face-3D</td>
              <td>0.074</td>
              <td>0.147</td>
              <td>181M</td>
              <td>21.5 ms/frame</td>
            </tr>
            <tr style="background: #eff6ff;">
              <td><strong>Facial Motion Generator (Ours)</strong></td>
              <td><strong>0.035</strong></td>
              <td><strong>0.111</strong></td>
              <td><strong>0.72M</strong></td>
              <td><strong>0.62 ms/frame</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- BibTeX -->
    <section>
      <div class="container">
        <h2>BibTex</h2>
        <div class="bibtex">
          <button class="copy-btn" onclick="copyBibtex()">Copy</button>
@inproceedings{ichikawa2026facial,
  title={3Dアバターに適用可能な音声駆動型リアルタイム表情生成システム},
  author={市川淳貴 and 徳久良子},
  booktitle={言語処理学会 第32回年次大会},
  year={2026}
}</div>
      </div>
    </section>

    <!-- Acknowledgments -->
    <section class="alt">
      <div class="container">
        <h2>謝辞</h2>
        <div style="max-width: 700px; margin: 0 auto; text-align: left;">
          <p style="margin-bottom: 20px;">
            本プロジェクトは以下のリソースを利用させていただきました。これらの優れたリソースを提供してくださった関係者の皆様に深く感謝いたします。
          </p>
          <ul style="list-style: none; padding: 0;">
            <li style="margin-bottom: 12px;">
              <strong><a href="https://github.com/NVIDIA/Audio2Face-3D" target="_blank">NVIDIA Audio2Face-3D</a></strong><br>
              <span style="color: #6b7280;">音声駆動型表情生成のフレームワークおよび学習データセットを利用させていただきました。</span>
            </li>
            <li>
              <strong><a href="https://github.com/mmdagent-ex/gene" target="_blank">MMDAgent-EX Gene</a></strong><br>
              <span style="color: #6b7280;">デモンストレーションにおける3Dアバターとして利用させていただきました。</span>
            </li>
          </ul>
        </div>
      </div>
    </section>
  </main>

  <footer>
    <div class="container">
      <p>
        © 2026 Junki Ichikawa, Ryoko Tokuhisa |
        <a href="https://github.com/cl-ait/facial-motion-generator">GitHub</a> |
        Licensed under <a href="https://opensource.org/licenses/MIT">MIT License</a>
      </p>
    </div>
  </footer>

  <script>
    function copyBibtex() {
      const bibtex = document.querySelector('.bibtex').innerText.replace('Copy', '').trim();
      navigator.clipboard.writeText(bibtex).then(() => {
        const btn = document.querySelector('.copy-btn');
        btn.textContent = 'Copied!';
        setTimeout(() => btn.textContent = 'Copy', 2000);
      });
    }
  </script>

</body>
</html>
